## Calculation of Accuracy

In machine learning, accuracy is one of the most important performance evaluation metrics for a classification model. The mathematical formula for calculating the accuracy of a machine learning model is 1 – (Number of misclassified samples / Total number of samples). If you want to learn how to evaluate the performance of a machine learning model by calculating its accuracy, this project is for you.

### Introduction to Accuracy in Machine Learning

Accuracy means the state of being correct or precise. For example, think of a group of friends who guessed the release of the next part of Avengers, and whoever guessed the date which is either the exact release date or closest to the release date is the most accurate one. So, the degree of being closer to a specific value is nothing but accuracy. In machine learning, it is one of the most important and widely used performance evaluation metrics for classification. If you’ve never used it before, below is a comprehensive tutorial on the calculation of accuracy in machine learning using Python.

### Calculation of Accuracy using Python

For the calculation of the accuracy of a classification model, we must first train a model for any classification-based problem. Then, we can calculate the accuracy of our trained model.

Many people often confuse accuracy and precision(another classification metric) with each other, accuracy is how close the predicted values are to the expected value, while precision is how close the predicted values are with each other.

### Output

```
0.965
```

### Summary

So this is how you can easily calculate the accuracy of a machine learning model based on the classification problem. This is one of the most important performance evaluation metrics for classification in machine learning. The mathematical formula for calculating the accuracy of a machine learning model is 1 – (Number of misclassified samples / Total number of samples).